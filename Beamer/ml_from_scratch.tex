\documentclass[notes]{beamer}

\mode<presentation>{\usetheme{Montpellier}}
%\usecolortheme{beaver}

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}
\newtheorem{thm}{Theorem}
\newtheorem{ra}{Ranking Algorithm}
\newtheorem{result}{Result}
\title{Machine Learning From Scratch}
\author{Collin Prather}
\date{April 25th, 2018}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\usepackage[style=british]{csquotes}

\def\signed #1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip1em
		\hbox{}\nobreak\hfill #1%
		\parfillskip=0pt \finalhyphendemerits=0 \endgraf}}

\newsavebox\mybox
\newenvironment{aquote}[1]
{\savebox\mybox{#1}\begin{quote}\openautoquote\hspace*{-.7ex}}
	{\unskip\closeautoquote\vspace*{1mm}\signed{\usebox\mybox}\end{quote}}


\begin{document}


	
\frame{\titlepage}

\note{\textbf{\Large Make sure that everyone launches binder or colab.google first!!}\\
Also, give them a walkthrough of the whole presentation and tell them what we'll be building and what we'll be predicting!\\
* we'll be talking about how to apply machine learning to problems!}

\section{Machine Learning Overview}
\frame{\tableofcontents[currentsection]}

\note{Pull a graph of google search trends indicating how terms like "Data Science" and "Machine Learning" have blown up. \\
	Try to form talk around hitting on the theoretical mathematical side of ML as well as the difficluties/complexities faced in Applied ML\\
	data + algorthms = predicting the future
	(it's really a lot more than this -- understanding context and how to frame the question (usually) from a business perspective is huge)\\
	classification v. regression\\
	supervised/unsupervised/reinforcement learning\\
		when talking on reinforcement learning, mention and recommend AlphaGo documentary (it's on netflix!)\\
		considerations/complexities in building ML models}

\begin{frame}{What is Machine Learning?}
\begin{center}
	\includegraphics[scale=.35]{Figures/mlcoursera1}
\end{center}
\begin{block}{Arthur Samuel:}
	Machine learning is ``Field of study that gives computers the ability to learn without being explicitly programmed".
\end{block}
\end{frame}

\note{Even according to the experts, the exact definifion of the field of machine learning is a bit fuzzy, but  As early as 1959, Arthur Samuel quote. .\\
	
\vspace{5pt}

also, include Ng's explanation from MLYearning!}

\begin{frame}{What is Machine Learning}
	data + algorithms = predicting the future\\
	combo of statistics, calculus, etc...
\end{frame}

\note{ML techniqes can be applied to a wide range of problems in diverse industries. In fact, ML has become ubiquitous in our everyday lives
	* Siri/ Amazon Alexa\\
	* Recommendation systems (amazon, netflix)\\
	* Fraud Detection\\
	* Disease diagnosis\\
	* Supply Chain Optimization}



\begin{frame}{According to Google...}
\begin{center}
	\includegraphics[scale=.55]{Figures/google_trends}
\end{center}
\end{frame}

\begin{frame}{What has caused this spike?}
	
\begin{enumerate}
	\item Data Availability
	\begin{itemize}
		\item ecommerce
		\item Iot (sensor data)
	\end{itemize}
	\item Computational Scale
	\begin{itemize}
		\item Moore's Law
	\end{itemize}
\end{enumerate}
\end{frame}
\note{The math that powers machine learning algorithms has been around for quite a few years... so what's changed?\\
1. Data Availability\\
2. Computational Scale (NG MLY 01 pg 10)\\
The rise of the big data era has given us access to astounding amounts of data. That phenomenon paired with with the exponential growth we've experienced in computational advances, has created the perfect storm for the emergence of the field of machine learning.}

\section{Steps in the Machine Learning Process}
\frame{\tableofcontents[currentsection]}

\subsection{Step 0: Identify The Problem}

\begin{frame}
\begin{center}
	\includegraphics[scale=.45]{Figures/gr_logo}
\end{center}
\note{Let's say that you work for the city of Grand Rapids, and you find that there are an increased number of hit and runs when the driver 1 was drinkin.\\
\textbf{Do some research, maybe find a way to graph this?? You can do it!}}
	
\end{frame}

\subsection{Step 1: Get the Data}

\begin{frame}{Get the Data}
	This may look like:
	\begin{itemize}
		\item<1-2> SQL query
		\item<1-2> CSV download
		\item<1-2> Web-scraping
		\item<1-2> Designing experiments/surveys and collecting data yourself
	\end{itemize}

	\vspace{10pt}

	\only<2->{In our case, we'll head to \href{http://grdata-grandrapids.opendata.arcgis.com}{\color{blue}\underline{GRData}}.}
\end{frame}
\note{Obtaining the data you'll need looks very different depending on what domain you're working in. In some instances, it can be fairly simple and straightforward, for example, In a business context, most often it will require querying some sort of internal database. Could also be downloading a csv file. In other instances, it may require a bit more creativity -- For particular social research, you may need to scrape the web. In some cases, you may even need to collect some data yourself! Here are two examples:\\
1. You're developing a new data product at your company and are collecting data to fuel it\\
2. You're in public health and are working to make healthcare accessible to all residents of the greater GR area. You may need to conduct your own research to identify what may be inhibitting people from reaching healthcare.\\
In our case, we're lucky enough to have access to a meticulously maintained public database on the city of GR: GRData. Scroll through, show them the site and data, etc.}

\begin{frame}{Get the Data}
	In our case, we'll head to \href{http://grdata-grandrapids.opendata.arcgis.com}{\color{blue}\underline{GRData}}
	
	\vspace{10pt}
	
	\includegraphics[scale=.35]{Figures/crash_head}
\end{frame}
\note{After downloading the csv file, we can read the file into a pandas dataframe and explore it in our Jupyter notebook.\\
* note something about how we'll often refer to each row as an observation and each column as a feature}


\subsection{Step 2: Data Exploration}

\begin{frame}{Explore the Data}
	\begin{itemize}
		\item Verify data
		\item Visualize data
		\item Identify patterns
		\item Give direction to analysis
	\end{itemize}
\end{frame}
\note{This is kind of an unstructured approach to understanding initial patterns in the data and potentially points of interest.This process isn’t meant to reveal every bit of information a dataset holds, but rather give you direction in your analysis and potentially give you clues in how to process/model the data.\cite{Data_exploration} Now, if you're just emailed a csv file, this step is especially crucial, and it may take you some time to explore the data, get a feeling for what you're dealing with. If you are analyzing data that you work with day in and day out, this "exploration" process may be a bit more implicit.\\
The main idea hear is to build an understanding of your data. Without an appreciation for the context of the data, it's just numbers. But when you see the data in context, it's fascinating, it's a story.\\
 More often than not, your exploration of the data leads to more questions than answers.}

\begin{frame}
\includegraphics[scale=.5]{Figures/coords}
\only<2>{\includegraphics[scale=.3]{Figures/coords_graphed}}
\end{frame}
\note{With our dataset, on car crashes, a logical place to begin would be the first two columns, containing latitudes and longitudes of each crash. This is just a snapshot of the data on the left side, and on the right, each dot represents a car crash.}

\begin{frame}
\includegraphics[scale=.23]{Figures/coords_graphed}
\includegraphics[scale=.21]{Figures/gr_map}
\end{frame}
\note{Now, this is pretty telling about our data, remember, there is nearly 73,000 crashes recorded, and if we juxtapose this plot with the city of GR, we actually see that the plot of crashes outline the city boundaries!}

\begin{frame}
\begin{center}
	\includegraphics[scale=.33]{Figures/coords_graphed_hitnrun}
\end{center}
\end{frame}
\note{We may be interested in hit and runs...\\
I should add a caption or title to this plot}

\begin{frame}{Check the variable's distribution}
	\includegraphics[scale=.35]{Figures/veh3}
\end{frame}
\note{It's also often very helpful to check the distribution of the different variables. For example, our dataset contains many characteristics about what it defines as "DRIVER1", "DRIVER2", "DRIVER3". When I first came across that, I was impressed, like that's some seriously accurate data! But upon further examination, we find that many of the "DRIVER2" and "DRIVER3" columns are empty or contain errors. (This makes sense... not all crashes involve 3 driver!)\\
When we check the counts of the different values found in the "VEH3" column, we see that over 67,000 of them are errors! Now, this doesn't mean that the column is useless, but in terms of building a predictive model, this column probably won't be much help, so as we'll see in the data processing step, we'll end up dropping it. }

\begin{frame}{Check the variable's distribution}
\includegraphics[scale=.35]{Figures/driver1}
\end{frame}
\note{Continuing on, here we check the distribution of the age's of all the "DRIVER1"s recorded in the dataset. In my head, I would think that age would be an important feature in car crashes. And so we check it out, and i appears that there are a ton of instances bunched up between 1 and 100... that makes sense... then there is also a decent cluster of observations around 1000 years old... that does not make sense.}

\begin{frame}{Check the variable's distribution}
\includegraphics[scale=.3]{Figures/driver1_num}
\end{frame}
\note{Nearly 9,000 driver 1's are recorded as being 999 years-old.. That is a good thing to know before trying to build a predictive model, as it serously skews the data. We'll address that in our data processing stage.\\

\vspace{3pt}

Next, we move onto data preparation. This is the stage where we make the final manipulations to our data before feeding it into our ML algorithm. Now, you could make an argument that preparing the data is the most important part of the machine learning workflow. After all it's the data that fuels the algorithm, garbage in, garbage out! It's been shown time and time again that more/bigger data beats a better algorithm everytime. Though it usually appears to be straightforward, this step can often reqiure a lot of creativity.\\
}


\subsection{Step 3: Data Preparation}

\begin{frame}{Data Selection}
Use as minimal features as possible\\

\vspace{10pt}

\begin{enumerate}
	\item Computationally efficient
	\item Easier to interpret
	\item Simpler is better
\end{enumerate}
\only<2>{\includegraphics[scale=.4]{Figures/data_selection}}
\end{frame}
\note{The first step in preparing the data is simply choosing which features (you can think of as columns in the spreadsheet) you'll want to use in your model.
As seen in our EDA some columns have a lot of missing data, and we'll drop them entirely.\\
It's generally regarded as best practice to use as minimal amount of features as possible, such that your predictive model still predicts as accurately as you need it to.\\
* computationally efficient\\
* more easily interpretable\\
* simpler is better\\
So how do we actually determine which features to use?, we can use various statistical tests, and even some algorithms to determine which features are going to be most relevant to predicting our target variable (which for us is whether or not the driver who caused the crash was drinking).We won't go into detail here.\\

When you are first beginning to iterate through different ml models, it is okay to kind of eyeball it, or use what you know about the context of the data to choose some features.\\
You'll see here the python code I've used to index our dataframe and select the feature's we'll use in building our predictive model.}


\begin{frame}{Feature Engineering}
\includegraphics[scale=.07]{Figures/ng}
	\begin{aquote}{Prof. Andrew Ng}
		Coming up with features is difficult, time-consuming, requires expert knowledge. ‘Applied machine learning’ is basically feature engineering.    
	\end{aquote}

\vspace{1cm}

\includegraphics[scale=.08]{Figures/domingo}
	\begin{aquote}{Prof. Pedro Domingo}
		At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.    
	\end{aquote}
\end{frame}
\note{So now we have our 13 features that we've chosen to use, and we could just send these raw features into our algorithm, and it may perform well, but it may not. It's importatnt to remember that our ultimate goal is to build a predictive model that can make accurate predictions on new/unseen observations. When all the data comes in from a car crash that just happened, we want to be able to accurately predict whether or not it was a drunk driver that caused it.\\
“Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.” - Jason Brownlee \cite{DS_blog}
\\
We acknowledge, however, that the data that's being recorded (from those UD10 reports) isn't necessarily guaranteed to accurately represent reality. Which is an important thing if we want to build accurate, stable predictive models.\\
The examples we've shown here are absolutely not exhaustive when it comes to feature engineering...}

\begin{frame}{Feature Engineering: \textit{transforming "hour" variable}}

\end{frame}

\begin{frame}{Feature Engineering: \textit{imputing missing ages} }
	content...
\end{frame}


\begin{frame}{Data Processing}
	Two general types of data to deal with:\\
	
	\vspace{10pt}
	
	\begin{itemize}
		\item Numerical variables (Quantitative)
		\begin{itemize}
			\item Driver 1 age, number of injuries, etc
		\end{itemize}
	
	\vspace{5pt}
	
		\item Categorical variables (Qualitative)
		\begin{itemize}
			\item Hit and run, motorcycle involved, etc
		\end{itemize}
	\end{itemize}
\end{frame}
\note{Now, we move onto processing the data that we've selcted. the data processing stage naturally diverges into two substeps: dealing with numerical variables, and dealing with categorical variables.\\
It's important to note that some of the preprocessing steps we'll talk about here may actually be necessary for you to do to get the data in a format where you can explore it.\\
Numerical variables are quantitative -- it's something you can measure. In our case, some numerical variables are Driver1 age, and number of injuries.\\
Categorical variables are qualitative, in our case, we have some binary categorical variables: like whether or not the crash was a hit and run, whether or not a mototcycle was involved. It's either one or the other. Variables can of course have multiple categories, for example, which car insurance provider you choose: (Nationwide, Statefarm, BlueCross BlueShield) There is obviously a lot of them, but all drivers fit into one of those categories... or at least they should.\\
There is some grey area between the two... maybe mention speedlimits or hour of day?}

\begin{frame}{Data Processing: \textit{numerical variables}}
	
\end{frame}

\subsection{Step 4: Model Selection}


\subsection{Step 5: Cross-validation/Hyper-parameter tuning}

\section{Building a Support Vector Machine from Scratch}

\frame{\tableofcontents[currentsection]}

\section{Exploring Scikit-Learn and applying to GR Crash dataset}
\frame{\tableofcontents[currentsection]}


\begin{frame}
	\begin{thebibliography}{9}
		\bibitem{Data_exploration} https://www.sisense.com/glossary/data-exploration/
		\bibitem{DS_blog}
		https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63
	\end{thebibliography}
\end{frame}






\end{document}