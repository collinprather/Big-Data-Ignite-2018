\documentclass[notes]{beamer}

\mode<presentation>{\usetheme{Montpellier}}
%\usecolortheme{beaver}

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}
\newtheorem{thm}{Theorem}
\newtheorem{ra}{Ranking Algorithm}
\newtheorem{result}{Result}
\title{Machine Learning From Scratch}
\author{Collin Prather}
\date{April 25th, 2018}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\usepackage[style=british]{csquotes}

\def\signed #1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip1em
		\hbox{}\nobreak\hfill #1%
		\parfillskip=0pt \finalhyphendemerits=0 \endgraf}}

\newsavebox\mybox
\newenvironment{aquote}[1]
{\savebox\mybox{#1}\begin{quote}\openautoquote\hspace*{-.7ex}}
	{\unskip\closeautoquote\vspace*{1mm}\signed{\usebox\mybox}\end{quote}}


\begin{document}


	
\frame{\titlepage}

\note{\textbf{\Large Make sure that everyone launches binder or colab.google first!!}\\
Also, give them a walkthrough of the whole presentation and tell them what we'll be building and what we'll be predicting!\\
* we'll be talking about how to apply machine learning to problems!\\
* mention the applied v. theoretical contrast. \\
Part 1) Applied Machine Learning case study: Drunk Driving Car Crashes\\
PArt 2) [We totally change gears] SVM from scratch that exemplifies/applies the general math process that each ML project follows (by that I mean: Representation, Evaluation, Optimization)\\
The main goal here is to expose you to a wide spectrum of technical/theoretical machine learning considerations in as applied of a setting as possible.}

\begin{frame}{Special Thanks}
	Dr. D\\
	Dr. Michael Bloem
\end{frame}
\note{I'd like to offer a very special thank you to Dr.Devereaux and Dr. Michael Bloem for all that they've done to help me in putting this together -- would not be possible without them!}

\section{Machine Learning Overview}
\frame{\tableofcontents[currentsection]}

\note{Pull a graph of google search trends indicating how terms like "Data Science" and "Machine Learning" have blown up. \\
	Try to form talk around hitting on the theoretical mathematical side of ML as well as the difficluties/complexities faced in Applied ML\\
	data + algorthms = predicting the future
	(it's really a lot more than this -- understanding context and how to frame the question (usually) from a business perspective is huge)\\
	classification v. regression\\
	supervised/unsupervised/reinforcement learning\\
		when talking on reinforcement learning, mention and recommend AlphaGo documentary (it's on netflix!)\\
		considerations/complexities in building ML models}

\begin{frame}{What is Machine Learning?}
\begin{center}
	\includegraphics[scale=.35]{Figures/mlcoursera1}
\end{center}
\begin{block}{Arthur Samuel:}
	Machine learning is ``Field of study that gives computers the ability to learn without being explicitly programmed".
\end{block}
\end{frame}

\note{Even according to the experts, the exact definifion of the field of machine learning is a bit fuzzy, but  As early as 1959, Arthur Samuel quote. .\\
	
\vspace{5pt}

also, include Ng's explanation from MLYearning!}

\begin{frame}{What is Machine Learning}
	data + algorithms = predicting the future\\
	combo of statistics, calculus, etc...
\end{frame}

\note{ML techniqes can be applied to a wide range of problems in diverse industries. In fact, ML has become ubiquitous in our everyday lives
	* Siri/ Amazon Alexa\\
	* Recommendation systems (amazon, netflix)\\
	* Fraud Detection\\
	* Disease diagnosis\\
	* Supply Chain Optimization}



\begin{frame}{According to Google...}
\begin{center}
	\includegraphics[scale=.55]{Figures/google_trends}
\end{center}
\end{frame}

\begin{frame}{What has caused this spike?}
	
\begin{enumerate}
	\item Data Availability
	
	\vspace{10pt}
	
	\begin{itemize}
		\item digital data
		
		\vspace{10pt}
		
		\item Iot (sensor data)
		
		\vspace{10pt}
		
	
	\end{itemize}
	\item Computational Scale
	
	\vspace{10pt}
	
	\begin{itemize}
		\item Moore's Law
	\end{itemize}
\end{enumerate}
\end{frame}
\note{The math that powers machine learning algorithms has been around for quite a few years... so what's changed?\\
1. Data Availability  (here we should just a few examples)\\
2. Computational Scale (NG MLY 01 pg 10)\\
The rise of the big data era has given us access to astounding amounts of data. That phenomenon paired with with the exponential growth we've experienced in computational advances, has created the perfect storm for the emergence of the field of machine learning.}

\section{Steps in the Machine Learning Process}
\frame{\tableofcontents[currentsection]}

\subsection{Step 0: Identify The Problem}

\begin{frame}
\begin{center}
	\includegraphics[scale=.45]{Figures/gr_logo}
\end{center}
\note{Setp 0 is to identify a problem that can be framed in such a way that it can be solved using machine learning.\\
Let's say that you work for the city of Grand Rapids, and you find that there are an increased number of hit and runs when the driver 1 was drinkin.\\
\textbf{Do some research, maybe find a way to graph this?? You can do it!}}
	
\end{frame}

\subsection{Step 1: Get the Data}

\begin{frame}{Get the Data}
	This may look like:
	
	\vspace{10pt}
	
	\begin{itemize}
		\item<1-2> SQL query
		
		\vspace{5pt}
		
		\item<1-2> CSV download
		
		\vspace{5pt}
		
		\item<1-2> Web-scraping
		
		\vspace{5pt}
		
		\item<1-2> Designing experiments/surveys and collecting data yourself
	\end{itemize}

	\vspace{10pt}

	\only<2->{In our case, we'll head to \href{http://grdata-grandrapids.opendata.arcgis.com}{\color{blue}\underline{GRData}}.}
\end{frame}
\note{Step 1! Obtaining the data you'll need looks very different depending on what domain you're working in. In some instances, it can be fairly simple and straightforward, for example, In a business context, most often it will require querying some sort of internal database. Could also be downloading a csv file. In other instances, it may require a bit more creativity -- For particular social research, you may need to scrape the web. In some cases, you may even need to collect some data yourself! Here are two examples:\\
1. You're developing a new data product at your company and are collecting data to fuel it\\
2. You're in public health and are working to make healthcare accessible to all residents of the greater GR area. You may need to conduct your own research to identify what may be inhibitting people from reaching healthcare.\\
In our case, we're lucky enough to have access to a meticulously maintained public database on the city of GR: GRData. Scroll through, show them the site and data, etc.}

\begin{frame}{Get the Data}
	In our case, we'll head to \href{http://grdata-grandrapids.opendata.arcgis.com}{\color{blue}\underline{GRData}}
	
	\vspace{10pt}
	
	\includegraphics[scale=.35]{Figures/crash_head}
\end{frame}
\note{After downloading the csv file, we can read the file into a pandas dataframe and explore it in our Jupyter notebook.\\
* note something about how we'll often refer to each row as an observation and each column as a feature}


\subsection{Step 2: Data Exploration}

\begin{frame}{Explore the Data}
	\begin{itemize}
		\item Verify data
		
		\vspace{5pt}
		
		\item Visualize data
		
		\vspace{5pt}
		
		\item Identify patterns
		
		\vspace{5pt}
		
		\item Give direction to analysis
	\end{itemize}
\end{frame}
\note{Step 2! This is kind of an unstructured approach to understanding initial patterns in the data and potentially points of interest.This process isn’t meant to reveal every bit of information a dataset holds, but rather give you direction in your analysis and potentially give you clues in how to process/model the data.\cite{Data_exploration} Now, if you're just emailed a csv file, this step is especially crucial, and it may take you some time to explore the data, get a feeling for what you're dealing with. If you are analyzing data that you work with day in and day out, this "exploration" process may be a bit more implicit.\\
The main idea hear is to build an understanding of your data. Without an appreciation for the context of the data, it's just numbers. But when you see the data in context, it's fascinating, it's a story.\\
 More often than not, your exploration of the data leads to more questions than answers.}

\begin{frame}
\includegraphics[scale=.45]{Figures/coords}
\only<2>{\includegraphics[scale=.3]{Figures/coords_graphed}}
\end{frame}
\note{With our dataset, on car crashes, a logical place to begin would be the first two columns, containing latitudes and longitudes of each crash. This is just a snapshot of the data on the left side, and on the right, each dot represents a car crash.}

\begin{frame}
\includegraphics[scale=.23]{Figures/coords_graphed}
\includegraphics[scale=.21]{Figures/gr_map}
\end{frame}
\note{Now, this is pretty telling about our data, remember, there is nearly 73,000 crashes recorded, and if we juxtapose this plot with the city of GR, we actually see that the plot of crashes outline the city boundaries!}

\begin{frame}
\begin{center}
	\includegraphics[scale=.33]{Figures/coords_graphed_hitnrun}
\end{center}
\end{frame}
\note{We may be interested in hit and runs...\\
I should add a caption or title to this plot}

\begin{frame}{Check the variable's distribution}
	\includegraphics[scale=.35]{Figures/veh3}
\end{frame}
\note{It's also often very helpful to check the distribution of the different variables. For example, our dataset contains many characteristics about what it defines as "DRIVER1", "DRIVER2", "DRIVER3". When I first came across that, I was impressed, like that's some seriously accurate data! But upon further examination, we find that many of the "DRIVER2" and "DRIVER3" columns are empty or contain errors. (This makes sense... not all crashes involve 3 driver!)\\
When we check the counts of the different values found in the "VEH3" column, we see that over 67,000 of them are errors! Now, this doesn't mean that the column is useless, but in terms of building a predictive model, this column probably won't be much help, so as we'll see in the data processing step, we'll end up dropping it. }

\begin{frame}{Check the variable's distribution}
\includegraphics[scale=.35]{Figures/driver1}
\end{frame}
\note{Continuing on, here we check the distribution of the age's of all the "DRIVER1"s recorded in the dataset. In my head, I would think that age would be an important feature in car crashes. And so we check it out, and i appears that there are a ton of instances bunched up between 1 and 100... that makes sense... then there is also a decent cluster of observations around 1000 years old... that does not make sense.}

\begin{frame}{Check the variable's distribution}
\includegraphics[scale=.3]{Figures/driver1_num}
\end{frame}
\note{Nearly 9,000 driver 1's are recorded as being 999 years-old.. That is a good thing to know before trying to build a predictive model, as it serously skews the data. We'll address that in our data processing stage.\\

\vspace{3pt}

Next, we move onto data preparation. This is the stage where we make the final manipulations to our data before feeding it into our ML algorithm. Now, you could make an argument that preparing the data is the most important part of the machine learning workflow. After all it's the data that fuels the algorithm, garbage in, garbage out! It's been shown time and time again that more/bigger data beats a better algorithm everytime. Though it usually appears to be straightforward, this step can often reqiure a lot of creativity.\\
}


\subsection{Step 3: Data Preparation}

\begin{frame}{Data Selection}
Use as minimal features as possible\\

\vspace{10pt}

\begin{enumerate}
	\item Computationally efficient
	
	\vspace{5pt}
	
	\item Easier to interpret
	
	\vspace{5pt}
	
	\item Simpler is better
\end{enumerate}
\only<2>{\includegraphics[scale=.4]{Figures/data_selection}}
\end{frame}
\note{This kicks off step 3, Data Preparation! The first part in preparing the data is simply choosing which features (you can think of as columns in the spreadsheet) you'll want to use in your model.
As seen in our EDA some columns have a lot of missing data, and we'll drop them entirely.\\
It's generally regarded as best practice to use as minimal amount of features as possible, such that your predictive model still predicts as accurately as you need it to.\\
* computationally efficient\\
* more easily interpretable\\
* simpler is better\\
So how do we actually determine which features to use?, we can use various statistical tests, and even some algorithms to determine which features are going to be most relevant to predicting our target variable (which for us is whether or not the driver who caused the crash was drinking).We won't go into detail here.\\

When you are first beginning to iterate through different ml models, it is okay to kind of eyeball it, or use what you know about the context of the data to choose some features.\\
You'll see here the python code I've used to index our dataframe and select the feature's we'll use in building our predictive model.}


\begin{frame}{Feature Engineering}
\includegraphics[scale=.07]{Figures/ng}
	\begin{aquote}{Prof. Andrew Ng}
		Coming up with features is difficult, time-consuming, requires expert knowledge. ‘Applied machine learning’ is basically feature engineering.    
	\end{aquote}

\vspace{1cm}

\includegraphics[scale=.09]{Figures/domingo}
	\begin{aquote}{Prof. Pedro Domingos}
		At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.    
	\end{aquote}
\end{frame}
\note{So now we have our 13 features that we've chosen to use, and we could just send these raw features into our algorithm, and it may perform well, but it may not. It's importatnt to remember that our ultimate goal is to build a predictive model that can make accurate predictions on new/unseen observations. When all the data comes in from a car crash that just happened, we want to be able to accurately predict whether or not it was a drunk driver that caused it.\\
“Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.” - Jason Brownlee \cite{DS_blog}
\\
We acknowledge, however, that the data that's being recorded (from those UD10 reports) isn't necessarily guaranteed to accurately represent reality. Which is an important thing if we want to build accurate, stable predictive models.\\
The examples that we show here are absolutely not exhaustive when it comes to feature engineering...}

\begin{frame}{Feature Engineering: \textit{transforming "hour" variable}}
\only<1-3>{$\left[\begin{matrix}
	12\\
	2\\
	4\\
	18\\
	19\\
	6\\
	\vdots
\end{matrix}\right]$}
\only<2-3>{$\hspace{10pt}\Longrightarrow\hspace{10pt}f(x) = \frac{2\cdot\pi\cdot(\text{hour})}{24}$}
\only<3>{$\hspace{10pt}\Longrightarrow\hspace{10pt}\left[\begin{matrix}
	3.14\\
	0.52\\
	1.03\\
	4.71\\
	4.98\\
	1.57\\
	\vdots
	\end{matrix}\right]$}

\end{frame}
\note{It contains at what hour the crash occured at (ranging from 0-23). It did not make sense to me to treat this column as a numerical variable, since the model would interpret 2pm as twice 1pm... while that does not make logical sense. However, I felt that to treat each hour as it's own category wouldn't accurately capture the information that the hour conveys (for example, it would not capture the fact that 12pm and 12am are 12 hours apart, while 12pm and 1pm are next to each other), so I set out to transform this feature in order to convey more meaning to our algorithm. Here's what I did.}



\begin{frame}{Feature Engineering: \textit{transforming "hour" variable}}
\only<1>{\centering{$\left[\begin{matrix}
3.14\\
0.52\\
1.03\\
4.71\\
4.98\\
1.57\\
\vdots
\end{matrix}\right]$}}
\only<1>{\centering{$\hspace{10pt}\Longrightarrow\hspace{10pt}\underbrace{\left[\begin{matrix}
			0.00\\
			0.47\\
			0.86\\
			-0.99\\
			-0.97\\
			1.0\\
			\vdots
			\end{matrix}\right]}_{sin(f(x))},\ 
		\underbrace{\left[\begin{matrix}
				-1.0\\
				0.87\\
				0.51\\
				-0.002\\
				-0.26\\
				0.001\\
				\vdots
			\end{matrix}\right]}_{cos(f(x))}$}}
\end{frame}
\note{Okay, so we have this new vector.. it doesn't look too helpful at the moment. But now we map that vector to two other vectors}


\begin{frame}{Feature Engineering: \textit{transforming "hour" variable}}
	\includegraphics[scale=.37]{Figures/hour}
\end{frame}
\note{transferring that all into code...}



\begin{frame}{Feature Engineering: \textit{imputing missing ages} }
\only<1>{\includegraphics[scale=.3]{Figures/driver1_num}}
\only<2>{\includegraphics[scale=.3]{Figures/driver1_imputed}}
\end{frame}
\note{Now, earlier when we were exploring our data and checking the distribution of the ages, we found that there are about 9,000 erraneous ages.\\
When you have erraneous/outright missing data you have a couple different options.\\
1. drop them entirely\\
2. impute mean/median (make this choice depending on distribution of data)\\
For most cases, one of those two options will work just fine. In our case, with nearly 9,000 errors, I'm not sure any of those would make sense, so we'll opt for a bit more involved technique. (The technical term is "Multiple Imputation by Chained Equations", affectionately known as MICE). What this means is that we will fit a linear regression model to our data to predict what the missing ages could be. We can think of these as making an educated guess.}


\begin{frame}{Data Processing}
	Two general types of data to deal with:\\
	
	\vspace{10pt}
	
	\begin{itemize}
		\item Numerical variables (Quantitative)
		\begin{itemize}
			\item Driver 1 age, number of injuries, etc
		\end{itemize}
	
	\vspace{5pt}
	
		\item Categorical variables (Qualitative)
		\begin{itemize}
			\item Hit and run, motorcycle involved, etc
		\end{itemize}
	\end{itemize}
\end{frame}
\note{Now, we move onto processing the data that we've selcted. the data processing stage naturally diverges into two substeps: dealing with numerical variables, and dealing with categorical variables.\\
It's important to note that some of the preprocessing steps we'll talk about here may actually be necessary for you to do to get the data in a format where you can explore it.\\
Numerical variables are quantitative -- it's something you can measure. In our case, some numerical variables are Driver1 age, and number of injuries.\\
Categorical variables are qualitative, in our case, we have some binary categorical variables: like whether or not the crash was a hit and run, whether or not a mototcycle was involved. It's either one or the other. Variables can of course have multiple categories, for example, which car insurance provider you choose: (Nationwide, Statefarm, BlueCross BlueShield) There is obviously a lot of them, but all drivers fit into one of those categories... or at least they should.\\
There is some grey area between the two... maybe mention speedlimits or hour of day?}

\begin{frame}{Data Processing: \textit{numerical variables}}
\only<1>{\includegraphics[scale=.5]{Figures/numericals}}
\only<2>{\includegraphics[scale=.37]{Figures/numericals_scaled}}
\end{frame}
\note{Here, we have our numerical variables. One problem that we still face with these numerical variables is that when we feed our data through the algorithm, the computer will view an increase in 1 latitude of latitude as equivalent to an increase in 1 year of Age of the driver. In actuality and increase in 1 degree of latitude could land you in an entirely different neighborhood, while 37 y/o and 38 y/o are basically exactly the same.\\
We will rescale the numerical variables through a process called standardization. This is not necessary for all ml algorithms, but generally will not hurt if we do it.\\
When we standardize the variables, we rescale them so that they all have a mean of 0 and a S.D. of 1.\\
Notice that in this snippet, the number of injuries was 0 for all of them, and now is negative... this means that having 0 injuries in a car crash is below average...}

\begin{frame}{Data Processing: \textit{categorical variables}}
\only<1>{\includegraphics[scale=.35]{Figures/categoricals}}
\only<2>{\includegraphics[scale=.35]{Figures/categoricals_scaled}}
\end{frame}
\note{Here we have our categorical variables. In our excel spreadsheet, this looks great, nice and clean.., but if we want to squeeze this data into a model, we need to manipulate it into a format that makes sense for math. \\
Basically, we are going to turn all of our categories into 1's and 0's, representing yes' and no's. You can think of this as transforming the data to turn everything into a yes or no question.\\
Was the driver sex male? No. Was the driver sex female? yes.\\
I've seen this called creating dummy variables, or one-hot-encoding\\
This is just a snapshot, you see that our columns of categorical variables grew from 7 to 21\\
\textbf{Come up with more concrete explanation}}

\subsection{Step 4: Model Selection}

\begin{frame}{Choosing a Model/Representation}
\centering{\includegraphics[scale=.6]{Figures/ML_algos_table}}
\end{frame}
\note{And with that, our data is ready to be fed into a predictive model! The final step is to choose which predictive model to use. There are hundreds to choose from, and trade-offs associated with each. The good news is, as we alluded to earlier there are different types of machine learning problems and each come with their own set of algorithms -- so this narrows down our search considerably.\\
In our case, we're working on a supervised classification problem, so the upper left hand corner displays some common algorithms for problems like ours. It's very common to test a handful of them and choose the one that performs best on your dataset (or even combine some of them into an ensemble model.)\\
Remeber, the ultimate objective to choose a model that learns a predictive rule (which comes in the form of a equation) that can be generalized to new observations (both for classification and regression)}


\begin{frame}{Bias-Variance Tradeoff}
	\centering{\includegraphics[scale=.4]{Figures/bias_variance}}
\end{frame}
\note{One critical aspect to take into consideration when choosing a model is called the bias-variance tradeoff\\
So first, I'm going to define what I mean when I talk about the terms "bias" and "variance" in the context of machine learning, then we are going to look at some simplified examples (using this dataset here) in the regression context that I think really well convey the essence of applied machine learning:\\
\textbf{Maybe still show my graphs?}\\
\textbf{mention statistical techniques necessary to combat variance, like bagging}\\
maybe mention regularization?}


\subsection{Step 5: Cross-validation/Hyper-parameter tuning}

\section{Building a Support Vector Machine from Scratch}
\note{\textbf{Throughout this section, add notes for each of the headers in the jupyter notebook!}\\\textbf{Also, do not forget to look over Lin alg for SVM notebook!efz}}

\frame{\tableofcontents[currentsection]}

\subsection{Representation}
\note{\textbf{Representation}: Support Vector Machine\\}


\note{How do we find the optimal separating hyperplane?:\\
It appears that there can be many different separating hyperplanes (often there is infinite). How do we choose the optimal one? Remember, the goal of the SVM is to maximize the margin of the training data. In other words, we're going to choose the hyperplane that is as far away as possible from each class of datapoints.\\
We want to maximize the margin because It generalizes better to classifying unseen observations -> (meaning that it makes better predictions)\\
If we have our usual equation for a line: y=mx+b, then all we need to do is find that optimal $m$ and $b$ that characterize the optimal hyperplane!}


\note{Understanding the Definition of a hyperplane:\\
As we've seen previously, the separating hyperplane that we keep referring to (in 2-d) is just a line. I'm confident that you're familiar with the way we define lines:\\
Now, a quick refresher that b is just a constant that represents the y-intercept, m (usually defines the slope) is a coefficient to the variable x (also a constant) and the variable y even has a coefficient as well, in this case it's 1. With some algebraic manipulation, we get that:\\
So, that looks a bit unnatural.. As we've mentioned, this is the equation of a line (hyperplane) in 2-d, if we try to expand this equation y=mx+b to more and more dimensions(variables), it's not a real great form to express it in. So instead, we
choose to set the whole equation equal to zero, and represent all coefficients/constants with $\beta$, and all variables with $X_i$, so that we don't run out of variables to use.\\
In our Crash data, we use 14 variables... }

\subsection{Evaluation}
\note{\textbf{Evaluation}\\}

\subsection{Optimization}
\note{\textbf{Optimization}\\}

\section{Exploring Scikit-Learn and applying to GR Crash dataset}
\frame{\tableofcontents[currentsection]}


\begin{frame}
	\begin{thebibliography}{9}
		\bibitem{Data_exploration} https://www.sisense.com/glossary/data-exploration/
		\bibitem{DS_blog}
		https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63
		\bibitem{bias_variance_blog}
		http://scott.fortmann-roe.com/docs/BiasVariance.html
	\end{thebibliography}
\end{frame}






\end{document}