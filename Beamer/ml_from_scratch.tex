\documentclass[notes]{beamer}

\mode<presentation>{\usetheme{Montpellier}}
%\usecolortheme{beaver}

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}
\newtheorem{thm}{Theorem}
\newtheorem{ra}{Ranking Algorithm}
\newtheorem{result}{Result}
\title{Machine Learning From Scratch}
\author{Collin Prather}
\date{April 25th, 2018}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}

\usepackage[style=british]{csquotes}

\def\signed #1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip1em
		\hbox{}\nobreak\hfill #1%
		\parfillskip=0pt \finalhyphendemerits=0 \endgraf}}

\newsavebox\mybox
\newenvironment{aquote}[1]
{\savebox\mybox{#1}\begin{quote}\openautoquote\hspace*{-.7ex}}
	{\unskip\closeautoquote\vspace*{1mm}\signed{\usebox\mybox}\end{quote}}


\begin{document}


	
\frame{\titlepage}

\note{\textbf{\Large Make sure that everyone launches binder or colab.google first!!}\\
Also, give them a walkthrough of the whole presentation and tell them what we'll be building and what we'll be predicting!\\
* we'll be talking about how to apply machine learning to problems!\\
* mention the applied v. theoretical contrast. \\
Part 1) Applied Machine Learning case study: Drunk Driving Car Crashes\\
PArt 2) [We totally change gears] SVM from scratch that exemplifies/applies the general math process that each ML project follows (by that I mean: Representation, Evaluation, Optimization)\\
The main goal here is to expose you to a wide spectrum of technical/theoretical machine learning considerations in as applied of a setting as possible.}

\begin{frame}{Special Thanks}
	Dr. D\\
	Dr. Michael Bloem
\end{frame}
\note{I'd like to offer a very special thank you to Dr.Devereaux and Dr. Michael Bloem for all that they've done to help me in putting this together -- would not be possible without them!}

\section{Machine Learning Overview}
\frame{\tableofcontents[currentsection]}

\note{Pull a graph of google search trends indicating how terms like "Data Science" and "Machine Learning" have blown up. \\
	Try to form talk around hitting on the theoretical mathematical side of ML as well as the difficluties/complexities faced in Applied ML\\
	data + algorthms = predicting the future
	(it's really a lot more than this -- understanding context and how to frame the question (usually) from a business perspective is huge)\\
	classification v. regression\\
	supervised/unsupervised/reinforcement learning\\
		when talking on reinforcement learning, mention and recommend AlphaGo documentary (it's on netflix!)\\
		considerations/complexities in building ML models}

\begin{frame}{What is Machine Learning?}
\begin{center}
	\includegraphics[scale=.35]{Figures/mlcoursera1}
\end{center}
\begin{block}{Arthur Samuel:}
	Machine learning is ``Field of study that gives computers the ability to learn without being explicitly programmed".
\end{block}
\end{frame}

\note{Even according to the experts, the exact definifion of the field of machine learning is a bit fuzzy, but  As early as 1959, Arthur Samuel quote. .\\
	
\vspace{5pt}

also, include Ng's explanation from MLYearning!}

\begin{frame}{What is Machine Learning}
	data + algorithms = predicting the future\\
	combo of statistics, calculus, etc...
\end{frame}

\note{ML techniqes can be applied to a wide range of problems in diverse industries. In fact, ML has become ubiquitous in our everyday lives
	* Siri/ Amazon Alexa\\
	* Recommendation systems (amazon, netflix)\\
	* Fraud Detection\\
	* Disease diagnosis\\
	* Supply Chain Optimization}



\begin{frame}{According to Google...}
\begin{center}
	\includegraphics[scale=.55]{Figures/google_trends}
\end{center}
\end{frame}

\begin{frame}{What has caused this spike?}
	
\begin{enumerate}
	\item Data Availability
	
	\vspace{10pt}
	
	\begin{itemize}
		\item digital data
		
		\vspace{10pt}
		
		\item Iot (sensor data)
		
		\vspace{10pt}
		
	
	\end{itemize}
	\item Computational Scale
	
	\vspace{10pt}
	
	\begin{itemize}
		\item Moore's Law
	\end{itemize}
\end{enumerate}
\end{frame}
\note{The math that powers machine learning algorithms has been around for quite a few years... so what's changed?\\
1. Data Availability  (here we should just a few examples)\\
2. Computational Scale (NG MLY 01 pg 10)\\
The rise of the big data era has given us access to astounding amounts of data. That phenomenon paired with with the exponential growth we've experienced in computational advances, has created the perfect storm for the emergence of the field of machine learning.}

\section{Steps in the Machine Learning Process}
\frame{\tableofcontents[currentsection]}

\subsection{Step 0: Identify The Problem}

\begin{frame}
\begin{center}
	\includegraphics[scale=.45]{Figures/gr_logo}
\end{center}
\note{Setp 0 is to identify a problem that can be framed in such a way that it can be solved using machine learning.\\
Let's say that you work for the city of Grand Rapids, and you find that there are an increased number of hit and runs when the driver 1 was drinkin.\\
\textbf{Do some research, maybe find a way to graph this?? You can do it!}}
	
\end{frame}

\subsection{Step 1: Get the Data}

\begin{frame}{Get the Data}
	This may look like:
	
	\vspace{10pt}
	
	\begin{itemize}
		\item<1-2> SQL query
		
		\vspace{5pt}
		
		\item<1-2> CSV download
		
		\vspace{5pt}
		
		\item<1-2> Web-scraping
		
		\vspace{5pt}
		
		\item<1-2> Designing experiments/surveys and collecting data yourself
	\end{itemize}

	\vspace{10pt}

	\only<2->{In our case, we'll head to \href{http://grdata-grandrapids.opendata.arcgis.com}{\color{blue}\underline{GRData}}.}
\end{frame}
\note{Step 1! Obtaining the data you'll need looks very different depending on what domain you're working in. In some instances, it can be fairly simple and straightforward, for example, In a business context, most often it will require querying some sort of internal database. Could also be downloading a csv file. In other instances, it may require a bit more creativity -- For particular social research, you may need to scrape the web. In some cases, you may even need to collect some data yourself! Here are two examples:\\
1. You're developing a new data product at your company and are collecting data to fuel it\\
2. You're in public health and are working to make healthcare accessible to all residents of the greater GR area. You may need to conduct your own research to identify what may be inhibitting people from reaching healthcare.\\
In our case, we're lucky enough to have access to a meticulously maintained public database on the city of GR: GRData. Scroll through, show them the site and data, etc.}

\begin{frame}{Get the Data}
	In our case, we'll head to \href{http://grdata-grandrapids.opendata.arcgis.com}{\color{blue}\underline{GRData}}
	
	\vspace{10pt}
	
	\includegraphics[scale=.35]{Figures/crash_head}
\end{frame}
\note{After downloading the csv file, we can read the file into a pandas dataframe and explore it in our Jupyter notebook.\\
* note something about how we'll often refer to each row as an observation and each column as a feature}


\subsection{Step 2: Data Exploration}

\begin{frame}{Explore the Data}
	\begin{itemize}
		\item Verify data
		
		\vspace{5pt}
		
		\item Visualize data
		
		\vspace{5pt}
		
		\item Identify patterns
		
		\vspace{5pt}
		
		\item Give direction to analysis
	\end{itemize}
\end{frame}
\note{Step 2! This is kind of an unstructured approach to understanding initial patterns in the data and potentially points of interest.This process isn’t meant to reveal every bit of information a dataset holds, but rather give you direction in your analysis and potentially give you clues in how to process/model the data.\cite{Data_exploration} Now, if you're just emailed a csv file, this step is especially crucial, and it may take you some time to explore the data, get a feeling for what you're dealing with. If you are analyzing data that you work with day in and day out, this "exploration" process may be a bit more implicit.\\
The main idea hear is to build an understanding of your data. Without an appreciation for the context of the data, it's just numbers. But when you see the data in context, it's fascinating, it's a story.\\
 More often than not, your exploration of the data leads to more questions than answers.}

\begin{frame}
\includegraphics[scale=.45]{Figures/coords}
\only<2>{\includegraphics[scale=.3]{Figures/coords_graphed}}
\end{frame}
\note{With our dataset, on car crashes, a logical place to begin would be the first two columns, containing latitudes and longitudes of each crash. This is just a snapshot of the data on the left side, and on the right, each dot represents a car crash.}

\begin{frame}
\includegraphics[scale=.23]{Figures/coords_graphed}
\includegraphics[scale=.21]{Figures/gr_map}
\end{frame}
\note{Now, this is pretty telling about our data, remember, there is nearly 73,000 crashes recorded, and if we juxtapose this plot with the city of GR, we actually see that the plot of crashes outline the city boundaries!}

\begin{frame}
\begin{center}
	\includegraphics[scale=.33]{Figures/coords_graphed_hitnrun}
\end{center}
\end{frame}
\note{We may be interested in hit and runs...\\
I should add a caption or title to this plot}

\begin{frame}{Check the variable's distribution}
	\includegraphics[scale=.35]{Figures/veh3}
\end{frame}
\note{It's also often very helpful to check the distribution of the different variables. For example, our dataset contains many characteristics about what it defines as "DRIVER1", "DRIVER2", "DRIVER3". When I first came across that, I was impressed, like that's some seriously accurate data! But upon further examination, we find that many of the "DRIVER2" and "DRIVER3" columns are empty or contain errors. (This makes sense... not all crashes involve 3 driver!)\\
When we check the counts of the different values found in the "VEH3" column, we see that over 67,000 of them are errors! Now, this doesn't mean that the column is useless, but in terms of building a predictive model, this column probably won't be much help, so as we'll see in the data processing step, we'll end up dropping it. }

\begin{frame}{Check the variable's distribution}
\includegraphics[scale=.35]{Figures/driver1}
\end{frame}
\note{Continuing on, here we check the distribution of the age's of all the "DRIVER1"s recorded in the dataset. In my head, I would think that age would be an important feature in car crashes. And so we check it out, and i appears that there are a ton of instances bunched up between 1 and 100... that makes sense... then there is also a decent cluster of observations around 1000 years old... that does not make sense.}

\begin{frame}{Check the variable's distribution}
\includegraphics[scale=.3]{Figures/driver1_num}
\end{frame}
\note{Nearly 9,000 driver 1's are recorded as being 999 years-old.. That is a good thing to know before trying to build a predictive model, as it serously skews the data. We'll address that in our data processing stage.\\

\vspace{3pt}

Next, we move onto data preparation. This is the stage where we make the final manipulations to our data before feeding it into our ML algorithm. Now, you could make an argument that preparing the data is the most important part of the machine learning workflow. After all it's the data that fuels the algorithm, garbage in, garbage out! It's been shown time and time again that more/bigger data beats a better algorithm everytime. Though it usually appears to be straightforward, this step can often reqiure a lot of creativity.\\
}


\subsection{Step 3: Data Preparation}

\begin{frame}{Data Selection}
Use as minimal features as possible\\

\vspace{10pt}

\begin{enumerate}
	\item Computationally efficient
	
	\vspace{5pt}
	
	\item Easier to interpret
	
	\vspace{5pt}
	
	\item Simpler is better
\end{enumerate}
\only<2>{\includegraphics[scale=.4]{Figures/data_selection}}
\end{frame}
\note{This kicks off step 3, Data Preparation! The first part in preparing the data is simply choosing which features (you can think of as columns in the spreadsheet) you'll want to use in your model.
As seen in our EDA some columns have a lot of missing data, and we'll drop them entirely.\\
It's generally regarded as best practice to use as minimal amount of features as possible, such that your predictive model still predicts as accurately as you need it to.\\
* computationally efficient\\
* more easily interpretable\\
* simpler is better\\
So how do we actually determine which features to use?, we can use various statistical tests, and even some algorithms to determine which features are going to be most relevant to predicting our target variable (which for us is whether or not the driver who caused the crash was drinking).We won't go into detail here.\\

When you are first beginning to iterate through different ml models, it is okay to kind of eyeball it, or use what you know about the context of the data to choose some features.\\
You'll see here the python code I've used to index our dataframe and select the feature's we'll use in building our predictive model.}


\begin{frame}{Feature Engineering}
\includegraphics[scale=.07]{Figures/ng}
	\begin{aquote}{Prof. Andrew Ng}
		Coming up with features is difficult, time-consuming, requires expert knowledge. ‘Applied machine learning’ is basically feature engineering.    
	\end{aquote}

\vspace{1cm}

\includegraphics[scale=.09]{Figures/domingo}
	\begin{aquote}{Prof. Pedro Domingos}
		At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.    
	\end{aquote}
\end{frame}
\note{So now we have our 13 features that we've chosen to use, and we could just send these raw features into our algorithm, and it may perform well, but it may not. It's importatnt to remember that our ultimate goal is to build a predictive model that can make accurate predictions on new/unseen observations. When all the data comes in from a car crash that just happened, we want to be able to accurately predict whether or not it was a drunk driver that caused it.\\
“Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.” - Jason Brownlee \cite{DS_blog}
\\
We acknowledge, however, that the data that's being recorded (from those UD10 reports) isn't necessarily guaranteed to accurately represent reality. Which is an important thing if we want to build accurate, stable predictive models.\\
The examples that we show here are absolutely not exhaustive when it comes to feature engineering...}

\begin{frame}{Feature Engineering: \textit{transforming "hour" variable}}
\only<1-3>{$\left[\begin{matrix}
	12\\
	2\\
	4\\
	18\\
	19\\
	6\\
	\vdots
\end{matrix}\right]$}
\only<2-3>{$\hspace{10pt}\Longrightarrow\hspace{10pt}f(x) = \frac{2\cdot\pi\cdot(\text{hour})}{24}$}
\only<3>{$\hspace{10pt}\Longrightarrow\hspace{10pt}\left[\begin{matrix}
	3.14\\
	0.52\\
	1.03\\
	4.71\\
	4.98\\
	1.57\\
	\vdots
	\end{matrix}\right]$}

\end{frame}
\note{It contains at what hour the crash occured at (ranging from 0-23). It did not make sense to me to treat this column as a numerical variable, since the model would interpret 2pm as twice 1pm... while that does not make logical sense. However, I felt that to treat each hour as it's own category wouldn't accurately capture the information that the hour conveys (for example, it would not capture the fact that 12pm and 12am are 12 hours apart, while 12pm and 1pm are next to each other), so I set out to transform this feature in order to convey more meaning to our algorithm. Here's what I did.}



\begin{frame}{Feature Engineering: \textit{transforming "hour" variable}}
\only<1>{\centering{$\left[\begin{matrix}
3.14\\
0.52\\
1.03\\
4.71\\
4.98\\
1.57\\
\vdots
\end{matrix}\right]$}}
\only<1>{\centering{$\hspace{10pt}\Longrightarrow\hspace{10pt}\underbrace{\left[\begin{matrix}
			0.00\\
			0.47\\
			0.86\\
			-0.99\\
			-0.97\\
			1.0\\
			\vdots
			\end{matrix}\right]}_{sin(f(x))},\ 
		\underbrace{\left[\begin{matrix}
				-1.0\\
				0.87\\
				0.51\\
				-0.002\\
				-0.26\\
				0.001\\
				\vdots
			\end{matrix}\right]}_{cos(f(x))}$}}
\end{frame}
\note{Okay, so we have this new vector.. it doesn't look too helpful at the moment. But now we map that vector to two other vectors}


\begin{frame}{Feature Engineering: \textit{transforming "hour" variable}}
	\includegraphics[scale=.37]{Figures/hour}
\end{frame}
\note{transferring that all into code...}



\begin{frame}{Feature Engineering: \textit{imputing missing ages} }
\only<1>{\includegraphics[scale=.3]{Figures/driver1_num}}
\only<2>{\includegraphics[scale=.3]{Figures/driver1_imputed}}
\end{frame}
\note{Now, earlier when we were exploring our data and checking the distribution of the ages, we found that there are about 9,000 erraneous ages.\\
When you have erraneous/outright missing data you have a couple different options.\\
1. drop them entirely\\
2. impute mean/median (make this choice depending on distribution of data)\\
For most cases, one of those two options will work just fine. In our case, with nearly 9,000 errors, I'm not sure any of those would make sense, so we'll opt for a bit more involved technique. (The technical term is "Multiple Imputation by Chained Equations", affectionately known as MICE). What this means is that we will fit a linear regression model to our data to predict what the missing ages could be. We can think of these as making an educated guess.}


\begin{frame}{Data Processing}
	Two general types of data to deal with:\\
	
	\vspace{10pt}
	
	\begin{itemize}
		\item Numerical variables (Quantitative)
		\begin{itemize}
			\item Driver 1 age, number of injuries, etc
		\end{itemize}
	
	\vspace{5pt}
	
		\item Categorical variables (Qualitative)
		\begin{itemize}
			\item Hit and run, motorcycle involved, etc
		\end{itemize}
	\end{itemize}
\end{frame}
\note{Now, we move onto processing the data that we've selcted. the data processing stage naturally diverges into two substeps: dealing with numerical variables, and dealing with categorical variables.\\
It's important to note that some of the preprocessing steps we'll talk about here may actually be necessary for you to do to get the data in a format where you can explore it.\\
Numerical variables are quantitative -- it's something you can measure. In our case, some numerical variables are Driver1 age, and number of injuries.\\
Categorical variables are qualitative, in our case, we have some binary categorical variables: like whether or not the crash was a hit and run, whether or not a mototcycle was involved. It's either one or the other. Variables can of course have multiple categories, for example, which car insurance provider you choose: (Nationwide, Statefarm, BlueCross BlueShield) There is obviously a lot of them, but all drivers fit into one of those categories... or at least they should.\\
There is some grey area between the two... maybe mention speedlimits or hour of day?}

\begin{frame}{Data Processing: \textit{numerical variables}}
\only<1>{\includegraphics[scale=.5]{Figures/numericals}}
\only<2>{\includegraphics[scale=.37]{Figures/numericals_scaled}}
\end{frame}
\note{Here, we have our numerical variables. One problem that we still face with these numerical variables is that when we feed our data through the algorithm, the computer will view an increase in 1 latitude of latitude as equivalent to an increase in 1 year of Age of the driver. In actuality and increase in 1 degree of latitude could land you in an entirely different neighborhood, while 37 y/o and 38 y/o are basically exactly the same.\\
We will rescale the numerical variables through a process called standardization. This is not necessary for all ml algorithms, but generally will not hurt if we do it.\\
When we standardize the variables, we rescale them so that they all have a mean of 0 and a S.D. of 1.\\
Notice that in this snippet, the number of injuries was 0 for all of them, and now is negative... this means that having 0 injuries in a car crash is below average...}

\begin{frame}{Data Processing: \textit{categorical variables}}
\only<1>{\includegraphics[scale=.35]{Figures/categoricals}}
\only<2>{\includegraphics[scale=.35]{Figures/categoricals_scaled}}
\end{frame}
\note{Here we have our categorical variables. In our excel spreadsheet, this looks great, nice and clean.., but if we want to squeeze this data into a model, we need to manipulate it into a format that makes sense for math. \\
Basically, we are going to turn all of our categories into 1's and 0's, representing yes' and no's. You can think of this as transforming the data to turn everything into a yes or no question.\\
Was the driver sex male? No. Was the driver sex female? yes.\\
I've seen this called creating dummy variables, or one-hot-encoding\\
This is just a snapshot, you see that our columns of categorical variables grew from 7 to 21\\
\textbf{Come up with more concrete explanation}}

\subsection{Step 4: Model Selection}

\begin{frame}{Choosing a Model/Representation}
\begin{tabular}{|c|l|l|}\hline
	&\textbf{Classification}&\textbf{Regression}\\\hline
	\textbf{Supervised}&
	\tabitem Logistic Regression&\tabitem Linear Regression\\
	&\tabitem Naive-Bayes&\tabitem Decision Trees\\
	&\tabitem KNN&\tabitem Random Forests\\
	&\tabitem SVM&\\\hline
	\textbf{Unsupervised}&
	\tabitem Apriori&\tabitem PCA\\
	&\tabitem Hidden Markov Model&\tabitem K-means\\
	&&\tabitem SVD\\
	\hline
\end{tabular}
\end{frame}
\note{And with that, our data is ready to be fed into a predictive model! The final step is to choose which predictive model to use. There are hundreds to choose from, and trade-offs associated with each. The good news is, as we alluded to earlier there are different types of machine learning problems and each come with their own set of algorithms -- so this narrows down our search considerably.\\
In our case, we're working on a supervised classification problem, so the upper left hand corner displays some common algorithms for problems like ours. It's very common to test a handful of them and choose the one that performs best on your dataset (or even combine some of them into an ensemble model.)\\
Remeber, the ultimate objective to choose a model that learns a predictive rule (which comes in the form of a equation) that can be generalized to new observations (both for classification and regression)}


\begin{frame}{Bias-Variance Tradeoff}
	\centering{\includegraphics[scale=.4]{Figures/bias_variance}}
\end{frame}
\note{One critical aspect to take into consideration when choosing a model is called the bias-variance tradeoff\\
So first, I'm going to define what I mean when I talk about the terms "bias" and "variance" in the context of machine learning, then we are going to look at some simplified examples (using this dataset here) in the regression context that I think really well convey the essence of applied machine learning:\\
\textbf{Maybe still show my graphs?}\\
\textbf{mention statistical techniques necessary to combat variance, like bagging}\\
maybe mention regularization?}


\subsection{Step 5: Cross-validation/Hyper-parameter tuning}

\section{Building a Support Vector Machine from Scratch}
\note{\textbf{Throughout this section, add notes for each of the headers in the jupyter notebook!}\\\textbf{Also, do not forget to look over Lin alg for SVM notebook!efz}}

\note{Here we'll first be going through a general example of building an svm from scratch on a toy dataset, then apply some of Python's ml tools to our crash data set to predict whether or not a car crash was caused by a drunk driver!}

\frame{\tableofcontents[currentsection]}

\note{Here, we will begin by importing the necessary libraries -- (audience should just run these). So here is what our plotted data looks like.\\
And here is a snapshot of data set. Note that for each data point (each row/each dot), we have an $x_1$ value, and $X_2$ value, and a $y$ value (which represents the target variable, which we are trying to predict). In this case, we've rather defined the red data points to be labeled as -1 annd the blue data points to be labeled as 1. It may seem weird to call them $x_1$ and $x_2$, but we'll address why we do that in a little bit.}

\subsection{Representation}
\note{\textbf{Representation}: Support Vector Machine\\
The first step in building any sort of predictive model is choosing a representation. For our example today, we choose a Support Vector Machine. Big picture - The goal of a SVM is to To find the optimal separating hyperplane which maximizes the margin of the training data.\\ 
Now, a hyperplane is kind of like a fancy word for a line, or maybe better put, it's a general way to talk about a line. The black line on this graph of our dataset is a hyperplane that separate the two classes (red/blue). Since our data is two dimensional (we have an x, and a y (1, 2)) the hyperplane is a line}


\note{How do we find the optimal separating hyperplane?:\\
Now, as we mentioned, the goal of the hyperplane is to find the optimal separating hyperplane, so how do we do that? It appears that there can be many different separating hyperplanes (often there is infinite). We're going to choose the hyperplane that is as far away as possible from each class of datapoints.\\
We want to maximize the margin because It generalizes better to classifying unseen observations -> (meaning that it makes better predictions)\\
If we have our usual equation for a line: y=mx+b, then all we need to do is find that optimal $m$ and $b$ that characterize the optimal hyperplane!}


\note{Understanding the Definition of a hyperplane:\\
As we've seen previously, the separating hyperplane that we keep referring to (in 2-d) is just a line. I'm confident that you're familiar with the way we define lines:\\
Now, a quick refresher that b is just a constant that represents the y-intercept, m (usually defines the slope) is a coefficient to the variable x (also a constant) and the variable y even has a coefficient as well, in this case it's 1. if we try to expand this equation y=mx+b to more and more dimensions(variables), it's not a real great form to express it in. So instead, we
choose to set the whole equation equal to zero. With some algebraic manipulation, we get that:\\
So, that looks a bit unnatural..so its common convention to represent all coefficients/constants with $\beta$, and all variables with $X_i$, so that we don't run out of variables to use.\\
In our Crash data, we use 14 variables...\\
\textbf{maybe change the betas to w's?} }

\subsection{Evaluation}
\note{\textbf{Evaluation}\\}

\note{\textbf{Quick refresher on the dot product:}\\
Now, we're going to talk for a minute about the dot product. The dot product has many different names across fields of mathematics: inner product, and linear combination are also common. Regardless of its name, it is a useful operation to use between vectors. To put it simply, if you take the dot product of two vectors, you multiply all their corresponding elements and take the sum. (note that the vectors must have the same dimensions).\\
The output of the dot product is not another vector, but just a scalar value.}

\note{\textbf{Dot product as distance from hyperplane:}\\
Now you'll notice that this dot product indeed looks very similar to the way that we've defined the equation of a hyperplane. It turns out that, when we take the dot product of our weights vector $\overrightarrow{w}$ and a single datapoint $\overrightarrow{x^{(i)}}$, the resulting number that we get can be thought of as the distance from the data point to the hyperplane (how far away it is). And since we're trying to maximize the width of our margin, having an efficient way to calculate how far away each datapoint is from the hyperplane proves to be cruical.\\
 Now, if we're being mathematically rigorous, we may want to frame that a bit differently, but at least for all intensive purposes, we can think of it this way!\\
The fact that we've defined the equation of a hyperplane to be the dot product of our weights and features equal to zero implies that the hyperplane and the weights vectors are perpendicular (aka orthogonal). And since they are orthogonal, we're able to exploit some linear algebra constructs, and it is on those grounds that say that the dot product is equal to the distance from the hyperplane.}

\note{\textbf{The Hinge Loss Function is defined as:}\\
Now, armed with this ability to calculate how far away each data pointis from the hyperplane, we can now use the Hinge Loss Function to quantify how wrong each of our predictions are!\\
One more important thing to note about the dot product is that it is signed, meaning that it can be either positive or negative, namely, data points below the hyperplane will have a negative distance from the hyperplane, and datapoints above the hyperplane, will have a positive distance from hyperplane.\\
Talk about how we will predict which class each data point comes from, then use the hinge loss to tell us how wrong we are. }

\note{\textbf{The Hinge Loss Function is defined as (Part 2): Breaking it down}\\
 1 minus $y$ times the dot product of the weights vector and the datapoint itself.\\
 Let's break this down piece by piece. First, we'll look at the inputs. $\overrightarrow{w}$ is a vector of weights (coefficients) in the linear equation. $\overrightarrow{x^{(i)}}$ is a vector representing a single datapoint (the $i^{th}$ datapoint), or a single row in our dataset. $y^{(i)}$ is the label associated with the datapoint (which is either 1 or -1). The ouput is a scalar value (a number) representing a penalty for how wrong our prediction was. The greater the penatly, the worse our estimated weights were in classifying the data.}


\note{\textbf{The Hinge Loss Function is defined as (Part 3): little subscript plus sign}\\
	One thing I've neglected to mention thus far is the little subscript plus sign at the end of the loss function. That little plus sign denotes that our loss function only looks at positive values, meaning that the output of function is a negative value, it's just going to change it to zero. This intuitively makes sense! If we get a negative penalty for our algorithm... that means that the prediction must be very correct, so instead, we'll just assign a penalty of 0, basically no penalty at all.\\}


\note{\textbf{The Hinge Loss Function is defined as (Part 4): working it out}\\
	Now, if you actually try to work this out with the datapoints, it can be a bit confusing, as there are lots of negatives and the sign of the function is constantly changing, so I've included this little table that works out small exmaples for all 4 cases, when the data point is correctly and incorrectly classified from the negative class, and when the data point was correctly and incorrectly classified from the postive class. For these little examples, I've assumed that the datapoint is 3 units away from the hyperplane. You'll notice if you skip to the bottom that the penalty for both incorrectly classified datapoints is 4, while the penalty for the correctly classified datapoints is 0.}
	
	
\note{\textbf{The Hinge Loss Function is defined as (Part 5): essence of ml}\\
	This is really worth emphasizing, because it doesn't just apply to SVM's but is the basis of machine learning. Machines learn by associating a quantitative penalty to incorrect predictions. The machine then sets out to minimize the penalty, which ultimately will result in making the maximium amount of correct predictions.\\
	 This is the essence of the current state of artificial intelligence. All AI is based on this principle, everything from autonomous vehicles, to amazon's recommendation system.}

\note{\textbf{Hinge Loss with regularization term:}\\
Now, with the hinge loss function we've defined, when we try to minimize the loss function, we will (if possible) find a separating hyperplane that perfectly classifies the data i.e. no misclaassifications. Which sounds great! The only problem with that is that is there are any outliers in our dataset, it can seriously skew the hyperplane. Below we have two nearly identical datasets...the one on the right has one outlier (circle it with mouse) and it has completely changed our hyperplane.\\
Again, we need to remember that we our goal is to find a hyperplane that will best classify new datapoints...so we don't want it to be affected by outliers.\\
The $\lambda$ coefficient acts as a tuning parameter (in our example, we will explicitly, hard-code a value for lambda), but generally, as the value for lambda gets smaller and smaller (approaches 0), the margin grows wider (to the point of misclassifying data points). As the value for lambda gets larger and larger, the margin will get smaller and smaller (to the point of no misclassified data points.)}

\subsection{Optimization}
\note{\textbf{Optimization}\\}

\note{\textbf{Optimization}\\
Now, we've been talking quite a bit about minimizing the penalty to our algorithm for mis-classifying data points, but it is not immediately obvious how to minimize the penalty.\\
We're going to use an algorithm called Stochastic Gradient Descent that will iteratively find the weights that minimize the total "loss" or "cost" to our Support Vector Machines, resulting in the very best possible predictions. \textbf{Look at this visually in two ways! 1. With stanford web demo, 2. with gradient descent drawing}}

\note{\textbf{Stanford Web App}\\
For one of its course, Stanford has created this awesome web demo that actively animates gradient descent. Here, they have three different classes (red, blue, green), while we only have two (red, blue), yet the principles are exactly the same. Gradient Descent begins with totally random separating boundaries, this means that we begin with random weights, $\overrightarrow{w}$, random coefficients in the equation of the separating hyperplane. (click randomize a few times)\\
The algorithm then iteratively updates the weights little by little until the separating hyperplane is correctly classifying the data points.(start repeated update with softmax)\\
So this is how gradient descent works.}

\note{\textbf{Gradient Descent Drawing}\\
I want to show you another angle we can look at this from. Take a look at this drawing. Imagine that this red line is the hinge loss function (preface: it's not. But it is a representative of the general concept of gradient descent), which, remember, outputs how wrong our predictions are, so we want to minimize this function, by finding the weights that correspond to the smallest possible output. All we have to do is move towards the bottom of this function, which is exactly what gradient descent does.
}

\note{\textbf{Transition to Math of GD}\\
Pragmatically speaking, the algorithm is going to loop through each observation (row) in our dataset, and check if our current weights would have correctly or incorrectly classified it. If it would have incorrectly classified the data point, then the algorithm updates the weights accordingly.\\
A natural question to ask then is, how are we going to update the weights? How do we move from the top of the function to the bottom? We will do so by using partial derivatives of the hinge loss function. We do this because:\\
 - The derivative of the hinge loss function gives us the slope of our hinge loss function at our current values for $\overrightarrow{w}$. Given the slope, we want to "move" or update our weights in the direction that the slope is decreasing i.e heading towards the bottom of the function.}

\note{\textbf{Calculating the partial derivatives:}\\
Okay, so let's calculate the partial derivatives. Using a rule from calculus, we can separately calculate the partial derivative of the first and second terms of the hinge loss function (loss term and regularization term) separately.\\
As we can see here, the partial derivative of the loss term depends on whether the datapoint was correctly or incorrectly classified. (0 if correctly classified, or -y times x if incorrectly classified). The partial derivative of the regularization term is 2 times lambda times w.}


\note{\textbf{Update Rules:}\\
}

\note{\textbf{To do next:}\\
Watch Andrew Ng's explanation of stochastic gradient descent and take notes in order to explain!\\
Then also outline exactly how to calculate the gradient of the hinge loss function for misclassified and correctly classified datapoints, then explicitly show the update rules.}

\section{Exploring Scikit-Learn and applying to GR Crash dataset}
\frame{\tableofcontents[currentsection]}


\begin{frame}
	\begin{thebibliography}{9}
		\bibitem{Data_exploration} https://www.sisense.com/glossary/data-exploration/
		\bibitem{DS_blog}
		https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63
		\bibitem{bias_variance_blog}
		http://scott.fortmann-roe.com/docs/BiasVariance.html
	\end{thebibliography}
\end{frame}






\end{document}